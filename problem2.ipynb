{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "y__uH7wTyFMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# IFT6135 - Homework 1 - CNN for MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "jlGZRfSSx5op",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ã‰ric Girard, \n",
        "Camille Rochefort-Boulanger, \n",
        "Emad Takla"
      ]
    },
    {
      "metadata": {
        "id": "7mNHP2j-xLll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# READ ME"
      ]
    },
    {
      "metadata": {
        "id": "ioW3dzSMxSCE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run the code, the path for the data file (downloaded with the script given for the homework) need to be specified (see first cell in DATA section).\n",
        "\n",
        "Also, the script creates a folder named \"results\" and inserts some pkl files in it when the models are trained."
      ]
    },
    {
      "metadata": {
        "id": "pmm8fy8Jx2-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SET UP"
      ]
    },
    {
      "metadata": {
        "id": "8-zfywm9IoYU",
        "colab_type": "code",
        "outputId": "b7cee316-b288-476b-c995-ddb5b82aa12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import gzip\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(f\"Pytorch version is {torch.__version__}.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch version is 1.0.1.post2.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7j5Psx2A8jxy",
        "colab_type": "code",
        "outputId": "b495b988-4cd5-42f3-e17f-a7714313bf07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9wvAHRV6d45r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# spot to save your learning curves, and potentially checkpoint your models\n",
        "savedir = 'results'\n",
        "if not os.path.exists(savedir):\n",
        "    os.makedirs(savedir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTCyFISTHVVy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ]
    },
    {
      "metadata": {
        "id": "UFffD2HlHm1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MNIST data file\n",
        "f = gzip.open('mnist.pkl.gz')\n",
        "data_mnist = np.load(f, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MOgKifvaQuWt",
        "colab_type": "code",
        "outputId": "2457e049-c940-4fed-e44e-e0724a688b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# Data to tensors\n",
        "train_images = torch.tensor([np.array([r.reshape(28,28)]) for r in data_mnist[0][0]], dtype=torch.float32)\n",
        "train_labels = torch.tensor(data_mnist[0][1], dtype=torch.int64)\n",
        "\n",
        "valid_images = torch.tensor([np.array([r.reshape(28,28)]) for r in data_mnist[1][0]], dtype=torch.float32)\n",
        "valid_labels = torch.tensor(data_mnist[1][1], dtype=torch.int64)\n",
        "\n",
        "test_images = torch.tensor([np.array([r.reshape(28,28)]) for r in data_mnist[1][0]], dtype=torch.float32)\n",
        "test_labels = torch.tensor(data_mnist[1][1], dtype=torch.int64)\n",
        "\n",
        "print(\"train images:\", train_images.shape, \"train labels:\", train_labels.shape)\n",
        "print(\"valid images:\", valid_images.shape, \"valid labels:\", valid_labels.shape)\n",
        "print(\"test images:\", test_images.shape, \"test labels:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train images: torch.Size([50000, 1, 28, 28]) train labels: torch.Size([50000])\n",
            "valid images: torch.Size([10000, 1, 28, 28]) valid labels: torch.Size([10000])\n",
            "test images: torch.Size([10000, 1, 28, 28]) test labels: torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JWZfbrvAW8MD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Datasets\n",
        "train_set = TensorDataset(train_images, train_labels)\n",
        "valid_set = TensorDataset(valid_images, valid_labels)\n",
        "test_set = TensorDataset(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7Ef2efrYo90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DataLoaders\n",
        "n_scratch = 64\n",
        "batch_size=1000\n",
        "batch_size_eval=512\n",
        "\n",
        "# Subset to overfit\n",
        "indices = list(range(len(train_set)))\n",
        "scratch_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    # The sampler is an easy way to say that we're using the elements\n",
        "    # `indices[:n_scratch]` for this loader\n",
        "    sampler=SubsetRandomSampler(indices[:n_scratch]),\n",
        "    num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "# Training set\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda\n",
        ")\n",
        "\n",
        "# Validation set\n",
        "valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=batch_size_eval,\n",
        "    shuffle=True,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        ")\n",
        "\n",
        "# Test set\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size_eval,\n",
        "    #num_workers=1,\n",
        "    pin_memory=use_cuda,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GC_8l94VaxtE",
        "colab_type": "code",
        "outputId": "544e7b6b-4515-4db0-b268-2c080c3f2d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "for inputs, targets in scratch_loader:\n",
        "    print(f\"Shape of scratch {inputs.shape}.\")\n",
        "    img = inputs[0,0]\n",
        "    plt.imshow(img, cmap='Greys_r')\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of scratch torch.Size([64, 1, 28, 28]).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEBBFwnA/wHr5/Y/V2QzzAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Q_zCf99ubcYT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MODELS"
      ]
    },
    {
      "metadata": {
        "id": "F1Yr6rY2ye8u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The models CNN1, CNN2 and CNN3 are inspired from the HeavyNet model which was seen in IFT6390 class."
      ]
    },
    {
      "metadata": {
        "id": "ct2QlHGtZ5Nb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN1"
      ]
    },
    {
      "metadata": {
        "id": "g6Rd2rMcbfMo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN1(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=0)\n",
        "        self.fc = nn.Linear(128*5*5, 10)\n",
        "\n",
        "    def forward(self, xin):\n",
        "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = xin + F.relu(self.conv1(xin))\n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # x is [b7s, 32, 14, 14]\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2) \n",
        "        # x is [bs, 64, 7, 7]\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 128*5*5 ) # flatten\n",
        "        x = F.relu(self.fc(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrczX3jedkB9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN2"
      ]
    },
    {
      "metadata": {
        "id": "irD6K_tqdnbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN2(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # conv block 1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        \n",
        "        # conv block 2\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        \n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 32, 14, 14]\n",
        "        \n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 64, 7, 7]\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zwSJ4bYZ-PK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN3"
      ]
    },
    {
      "metadata": {
        "id": "RJvoimV3YA0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN3(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # conv block 1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # conv block 2\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        \n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 32, 14, 14]\n",
        "        \n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 64, 7, 7]\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8JiaqbIaBMe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Heavy Net"
      ]
    },
    {
      "metadata": {
        "id": "Oqdpe5f0ySI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the code seen in the IFT6390 class."
      ]
    },
    {
      "metadata": {
        "id": "mbdfZx3CaExO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HeavyNet(nn.Module):\n",
        "    \"\"\"A medium sized network that performs very well on MNIST.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # conv block 1\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        # conv block 2\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 32, 14, 14]\n",
        "        \n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.max_pool2d(x, 2) # x is [bs, 64, 7, 7]\n",
        "        \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        \n",
        "        x = F.relu(self.bn5(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GyCkqHcUjWwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TRAIN & TEST FUNCTIONS"
      ]
    },
    {
      "metadata": {
        "id": "fbTstRobdrH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Surrogate loss used for training\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fe-dCdNQdwV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model,train_loader, optimizer, epoch ):\n",
        "    \"\"\"Perform one epoch of training.\"\"\"\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    train_size = 0\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        inputs, target = inputs.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        train_size += len(inputs)\n",
        "        train_loss = loss_fn(output, target)\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    train_loss /= train_size\n",
        "    accuracy = float(correct) / train_size\n",
        "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      train_loss, correct, train_size, 100. * accuracy))\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPV-RNcxeBk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_size = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, target in test_loader:\n",
        "            inputs, target = inputs.to(device), target.to(device)\n",
        "            \n",
        "            # let them code what's here\n",
        "            output = model(inputs)\n",
        "            test_size += len(inputs)\n",
        "            test_loss += test_loss_fn(output, target).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= test_size\n",
        "    accuracy = correct / test_size\n",
        "    \"\"\"\n",
        "    print('\\nTest (or valid) set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_size,\n",
        "        100. * accuracy))\n",
        "    \"\"\"\n",
        "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, test_size, 100. * accuracy))\n",
        "    \n",
        "    return test_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OVXj-EBajcxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ]
    },
    {
      "metadata": {
        "id": "YC6kIO1gjsbg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overfitting with scratch dataset"
      ]
    },
    {
      "metadata": {
        "id": "UFcIkfUGWBga",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN1"
      ]
    },
    {
      "metadata": {
        "id": "Uh_JqevseIVR",
        "colab_type": "code",
        "outputId": "30d8a741-11fc-45bd-d2d7-7c8c4b24a396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        }
      },
      "cell_type": "code",
      "source": [
        "# Overfitting\n",
        "model = CNN1().to(device)\n",
        "model_evol = []\n",
        "lr = 0.05\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 50):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 28640/50000 (57%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 33996/50000 (68%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 34378/50000 (69%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 34493/50000 (69%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 34594/50000 (69%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 34620/50000 (69%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 34690/50000 (69%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 34680/50000 (69%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 34753/50000 (70%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 34757/50000 (70%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 34767/50000 (70%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 34805/50000 (70%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 34815/50000 (70%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 34827/50000 (70%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 34795/50000 (70%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 34843/50000 (70%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 34864/50000 (70%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 34829/50000 (70%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 34869/50000 (70%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 34879/50000 (70%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 34894/50000 (70%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 34890/50000 (70%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 34898/50000 (70%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 34897/50000 (70%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 34912/50000 (70%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 34902/50000 (70%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 34917/50000 (70%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 34923/50000 (70%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 34927/50000 (70%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 34928/50000 (70%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 34925/50000 (70%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 34927/50000 (70%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 34934/50000 (70%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 34938/50000 (70%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 34932/50000 (70%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 34942/50000 (70%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 34940/50000 (70%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 34941/50000 (70%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 34943/50000 (70%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 34936/50000 (70%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 34937/50000 (70%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 34939/50000 (70%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 34944/50000 (70%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 34944/50000 (70%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 34940/50000 (70%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 34943/50000 (70%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 34940/50000 (70%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 34942/50000 (70%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 34944/50000 (70%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 34946/50000 (70%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zW_Qe41dX0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN2"
      ]
    },
    {
      "metadata": {
        "id": "wfU46vTTddVg",
        "colab_type": "code",
        "outputId": "a9007c82-4834-4c67-dc4e-8dd3dc4778b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        }
      },
      "cell_type": "code",
      "source": [
        "# Overfitting\n",
        "model = CNN2().to(device)\n",
        "model_evol = []\n",
        "lr = 0.05\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 50):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 31746/50000 (63%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 47326/50000 (95%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 48516/50000 (97%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 48898/50000 (98%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 49077/50000 (98%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 49274/50000 (99%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 49344/50000 (99%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 49409/50000 (99%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 49507/50000 (99%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 49527/50000 (99%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 49609/50000 (99%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 49656/50000 (99%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 49701/50000 (99%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 49693/50000 (99%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 49717/50000 (99%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 49762/50000 (100%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 49782/50000 (100%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 49833/50000 (100%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 49832/50000 (100%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 49874/50000 (100%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 49888/50000 (100%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 49893/50000 (100%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 49905/50000 (100%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 49888/50000 (100%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 49918/50000 (100%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 49920/50000 (100%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 49959/50000 (100%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 49957/50000 (100%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 49945/50000 (100%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 49970/50000 (100%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 49976/50000 (100%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 49982/50000 (100%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 49983/50000 (100%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 49985/50000 (100%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 49985/50000 (100%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 49984/50000 (100%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 49984/50000 (100%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 49986/50000 (100%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 49997/50000 (100%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 49996/50000 (100%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 49997/50000 (100%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 49993/50000 (100%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AlBeSi-vaS1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN3"
      ]
    },
    {
      "metadata": {
        "id": "c-A2r6XiYNoe",
        "colab_type": "code",
        "outputId": "0bb3af61-55a8-4150-cc0f-5e10d4513f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        }
      },
      "cell_type": "code",
      "source": [
        "# Overfitting\n",
        "model = CNN3().to(device)\n",
        "model_evol = []\n",
        "lr = 0.05\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 50):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 45714/50000 (91%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 49111/50000 (98%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 49413/50000 (99%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 49547/50000 (99%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 49658/50000 (99%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 49721/50000 (99%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 49798/50000 (100%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 49843/50000 (100%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 49883/50000 (100%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 49907/50000 (100%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 49932/50000 (100%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 49955/50000 (100%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 49974/50000 (100%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 49982/50000 (100%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 49977/50000 (100%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 49988/50000 (100%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 49985/50000 (100%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yLCPDuwaaXhn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Heavy Net"
      ]
    },
    {
      "metadata": {
        "id": "Bv0Wu1VaWEpu",
        "colab_type": "code",
        "outputId": "e4316453-36ab-4ddf-bcc4-3f53a4ceb1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        }
      },
      "cell_type": "code",
      "source": [
        "# Overfitting\n",
        "model = HeavyNet().to(device)\n",
        "model_evol = []\n",
        "lr = 0.05\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 50):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 46131/50000 (92%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 49313/50000 (99%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 49531/50000 (99%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 49689/50000 (99%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 49761/50000 (100%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 49823/50000 (100%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 49893/50000 (100%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 49900/50000 (100%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 49930/50000 (100%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 49964/50000 (100%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 49978/50000 (100%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 49980/50000 (100%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 49990/50000 (100%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 49999/50000 (100%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 49998/50000 (100%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 50000/50000 (100%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c8zfPH4ueQDJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train dataset"
      ]
    },
    {
      "metadata": {
        "id": "3ScBTXXweUKq",
        "colab_type": "code",
        "outputId": "ae329904-9d30-46bf-c6f7-dc742b5f01c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8684
        }
      },
      "cell_type": "code",
      "source": [
        "model = CNN2().to(device)\n",
        "model_evol = []\n",
        "lr = 0.005\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'cnn2', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, valid_loader)\n",
        "    model_evol.append(model)\n",
        "    \n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    results['t_accuracy'].append(train_acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 17103/50000 (34%)\n",
            "\n",
            "Test set: Average loss: 2.1215, Accuracy: 6240/10000 (62%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 34670/50000 (69%)\n",
            "\n",
            "Test set: Average loss: 0.6332, Accuracy: 8174/10000 (82%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 42548/50000 (85%)\n",
            "\n",
            "Test set: Average loss: 0.3611, Accuracy: 8908/10000 (89%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 44501/50000 (89%)\n",
            "\n",
            "Test set: Average loss: 0.2994, Accuracy: 9111/10000 (91%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 45234/50000 (90%)\n",
            "\n",
            "Test set: Average loss: 0.2665, Accuracy: 9214/10000 (92%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 45744/50000 (91%)\n",
            "\n",
            "Test set: Average loss: 0.2372, Accuracy: 9297/10000 (93%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 46236/50000 (92%)\n",
            "\n",
            "Test set: Average loss: 0.2156, Accuracy: 9367/10000 (94%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 46556/50000 (93%)\n",
            "\n",
            "Test set: Average loss: 0.2012, Accuracy: 9415/10000 (94%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 46918/50000 (94%)\n",
            "\n",
            "Test set: Average loss: 0.1799, Accuracy: 9485/10000 (95%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 47234/50000 (94%)\n",
            "\n",
            "Test set: Average loss: 0.1627, Accuracy: 9541/10000 (95%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 47448/50000 (95%)\n",
            "\n",
            "Test set: Average loss: 0.1476, Accuracy: 9583/10000 (96%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 47733/50000 (95%)\n",
            "\n",
            "Test set: Average loss: 0.1399, Accuracy: 9592/10000 (96%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 47928/50000 (96%)\n",
            "\n",
            "Test set: Average loss: 0.1288, Accuracy: 9645/10000 (96%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 48049/50000 (96%)\n",
            "\n",
            "Test set: Average loss: 0.1189, Accuracy: 9659/10000 (97%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 48183/50000 (96%)\n",
            "\n",
            "Test set: Average loss: 0.1159, Accuracy: 9664/10000 (97%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 48289/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.1130, Accuracy: 9681/10000 (97%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 48365/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.1067, Accuracy: 9709/10000 (97%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 48530/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0982, Accuracy: 9725/10000 (97%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 48589/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0930, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 48664/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0940, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 48691/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0929, Accuracy: 9738/10000 (97%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 48746/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0869, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 48833/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0840, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 48887/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0843, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 48936/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0804, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 48933/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0766, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 49009/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0747, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 49059/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0725, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 49087/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0724, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 49055/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0708, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 49107/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0705, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 49138/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0682, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 49152/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0680, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 49164/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0680, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 49204/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0679, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 49167/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0649, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 49243/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0659, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 49288/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0624, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 49298/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0635, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 49313/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0616, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 49342/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0605, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 49354/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0610, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 49347/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0666, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 49366/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0635, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 49395/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0585, Accuracy: 9821/10000 (98%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 49355/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0591, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 49399/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0569, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 49436/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0584, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 49393/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 49438/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0590, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "epoch 50\n",
            "Train set: Average loss: 0.0000, Accuracy: 49427/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0549, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "epoch 51\n",
            "Train set: Average loss: 0.0000, Accuracy: 49459/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0579, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "epoch 52\n",
            "Train set: Average loss: 0.0000, Accuracy: 49417/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0558, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "epoch 53\n",
            "Train set: Average loss: 0.0000, Accuracy: 49493/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0563, Accuracy: 9844/10000 (98%)\n",
            "\n",
            "epoch 54\n",
            "Train set: Average loss: 0.0000, Accuracy: 49465/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0555, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "epoch 55\n",
            "Train set: Average loss: 0.0000, Accuracy: 49495/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9841/10000 (98%)\n",
            "\n",
            "epoch 56\n",
            "Train set: Average loss: 0.0000, Accuracy: 49533/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0546, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "epoch 57\n",
            "Train set: Average loss: 0.0000, Accuracy: 49510/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0575, Accuracy: 9835/10000 (98%)\n",
            "\n",
            "epoch 58\n",
            "Train set: Average loss: 0.0000, Accuracy: 49526/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0541, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "epoch 59\n",
            "Train set: Average loss: 0.0000, Accuracy: 49517/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0559, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "epoch 60\n",
            "Train set: Average loss: 0.0000, Accuracy: 49541/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0560, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "epoch 61\n",
            "Train set: Average loss: 0.0000, Accuracy: 49525/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0539, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "epoch 62\n",
            "Train set: Average loss: 0.0000, Accuracy: 49533/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0517, Accuracy: 9846/10000 (98%)\n",
            "\n",
            "epoch 63\n",
            "Train set: Average loss: 0.0000, Accuracy: 49587/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0547, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "epoch 64\n",
            "Train set: Average loss: 0.0000, Accuracy: 49591/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0516, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "epoch 65\n",
            "Train set: Average loss: 0.0000, Accuracy: 49592/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0598, Accuracy: 9819/10000 (98%)\n",
            "\n",
            "epoch 66\n",
            "Train set: Average loss: 0.0000, Accuracy: 49594/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "epoch 67\n",
            "Train set: Average loss: 0.0000, Accuracy: 49601/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9860/10000 (99%)\n",
            "\n",
            "epoch 68\n",
            "Train set: Average loss: 0.0000, Accuracy: 49615/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0496, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "epoch 69\n",
            "Train set: Average loss: 0.0000, Accuracy: 49619/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0521, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "epoch 70\n",
            "Train set: Average loss: 0.0000, Accuracy: 49609/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "epoch 71\n",
            "Train set: Average loss: 0.0000, Accuracy: 49644/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "epoch 72\n",
            "Train set: Average loss: 0.0000, Accuracy: 49655/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0525, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "epoch 73\n",
            "Train set: Average loss: 0.0000, Accuracy: 49656/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0495, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "epoch 74\n",
            "Train set: Average loss: 0.0000, Accuracy: 49679/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0499, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "epoch 75\n",
            "Train set: Average loss: 0.0000, Accuracy: 49673/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0541, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "epoch 76\n",
            "Train set: Average loss: 0.0000, Accuracy: 49675/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0497, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "epoch 77\n",
            "Train set: Average loss: 0.0000, Accuracy: 49685/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "epoch 78\n",
            "Train set: Average loss: 0.0000, Accuracy: 49689/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "epoch 79\n",
            "Train set: Average loss: 0.0000, Accuracy: 49662/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0551, Accuracy: 9842/10000 (98%)\n",
            "\n",
            "epoch 80\n",
            "Train set: Average loss: 0.0000, Accuracy: 49695/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "epoch 81\n",
            "Train set: Average loss: 0.0000, Accuracy: 49712/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "epoch 82\n",
            "Train set: Average loss: 0.0000, Accuracy: 49713/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0508, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "epoch 83\n",
            "Train set: Average loss: 0.0000, Accuracy: 49707/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "epoch 84\n",
            "Train set: Average loss: 0.0000, Accuracy: 49695/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0516, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "epoch 85\n",
            "Train set: Average loss: 0.0000, Accuracy: 49730/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0497, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "epoch 86\n",
            "Train set: Average loss: 0.0000, Accuracy: 49735/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0509, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "epoch 87\n",
            "Train set: Average loss: 0.0000, Accuracy: 49731/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0521, Accuracy: 9863/10000 (99%)\n",
            "\n",
            "epoch 88\n",
            "Train set: Average loss: 0.0000, Accuracy: 49743/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0488, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "epoch 89\n",
            "Train set: Average loss: 0.0000, Accuracy: 49747/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0489, Accuracy: 9867/10000 (99%)\n",
            "\n",
            "epoch 90\n",
            "Train set: Average loss: 0.0000, Accuracy: 49765/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0493, Accuracy: 9863/10000 (99%)\n",
            "\n",
            "epoch 91\n",
            "Train set: Average loss: 0.0000, Accuracy: 49781/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0488, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "epoch 92\n",
            "Train set: Average loss: 0.0000, Accuracy: 49727/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "epoch 93\n",
            "Train set: Average loss: 0.0000, Accuracy: 49773/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0510, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "epoch 94\n",
            "Train set: Average loss: 0.0000, Accuracy: 49779/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "epoch 95\n",
            "Train set: Average loss: 0.0000, Accuracy: 49776/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0504, Accuracy: 9868/10000 (99%)\n",
            "\n",
            "epoch 96\n",
            "Train set: Average loss: 0.0000, Accuracy: 49788/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0491, Accuracy: 9865/10000 (99%)\n",
            "\n",
            "epoch 97\n",
            "Train set: Average loss: 0.0000, Accuracy: 49777/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0494, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "epoch 98\n",
            "Train set: Average loss: 0.0000, Accuracy: 49814/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0570, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "epoch 99\n",
            "Train set: Average loss: 0.0000, Accuracy: 49764/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0492, Accuracy: 9856/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DB77t5Whe6P_",
        "colab_type": "code",
        "outputId": "475af6c3-d093-4b61-bbcd-fd5ec07411fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8684
        }
      },
      "cell_type": "code",
      "source": [
        "model = CNN3().to(device)\n",
        "model_evol = []\n",
        "lr = 0.005\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "results = {'name':'cnn3', 'lr': lr, 'loss': [], 'accuracy':[], 't_accuracy':[]}\n",
        "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "    print ('epoch {}'.format(epoch))\n",
        "    train_acc = train(model, train_loader, optimizer, epoch)\n",
        "    loss, acc = test(model, valid_loader)\n",
        "    model_evol.append(model)\n",
        "    \n",
        "    # save results every epoch\n",
        "    results['loss'].append(loss)\n",
        "    results['accuracy'].append(acc)\n",
        "    results['t_accuracy'].append(train_acc)\n",
        "    with open(savefile, 'wb') as fout:\n",
        "        pickle.dump(results, fout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "Train set: Average loss: 0.0000, Accuracy: 39832/50000 (80%)\n",
            "\n",
            "Test set: Average loss: 0.4174, Accuracy: 9273/10000 (93%)\n",
            "\n",
            "epoch 1\n",
            "Train set: Average loss: 0.0000, Accuracy: 47527/50000 (95%)\n",
            "\n",
            "Test set: Average loss: 0.1619, Accuracy: 9693/10000 (97%)\n",
            "\n",
            "epoch 2\n",
            "Train set: Average loss: 0.0000, Accuracy: 48363/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.1169, Accuracy: 9765/10000 (98%)\n",
            "\n",
            "epoch 3\n",
            "Train set: Average loss: 0.0000, Accuracy: 48739/50000 (97%)\n",
            "\n",
            "Test set: Average loss: 0.0953, Accuracy: 9801/10000 (98%)\n",
            "\n",
            "epoch 4\n",
            "Train set: Average loss: 0.0000, Accuracy: 48933/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0836, Accuracy: 9812/10000 (98%)\n",
            "\n",
            "epoch 5\n",
            "Train set: Average loss: 0.0000, Accuracy: 49090/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0741, Accuracy: 9828/10000 (98%)\n",
            "\n",
            "epoch 6\n",
            "Train set: Average loss: 0.0000, Accuracy: 49190/50000 (98%)\n",
            "\n",
            "Test set: Average loss: 0.0678, Accuracy: 9839/10000 (98%)\n",
            "\n",
            "epoch 7\n",
            "Train set: Average loss: 0.0000, Accuracy: 49261/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0616, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "epoch 8\n",
            "Train set: Average loss: 0.0000, Accuracy: 49303/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0586, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "epoch 9\n",
            "Train set: Average loss: 0.0000, Accuracy: 49353/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "epoch 10\n",
            "Train set: Average loss: 0.0000, Accuracy: 49429/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0529, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "epoch 11\n",
            "Train set: Average loss: 0.0000, Accuracy: 49452/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0516, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "epoch 12\n",
            "Train set: Average loss: 0.0000, Accuracy: 49481/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0492, Accuracy: 9871/10000 (99%)\n",
            "\n",
            "epoch 13\n",
            "Train set: Average loss: 0.0000, Accuracy: 49527/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0494, Accuracy: 9867/10000 (99%)\n",
            "\n",
            "epoch 14\n",
            "Train set: Average loss: 0.0000, Accuracy: 49547/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0457, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "epoch 15\n",
            "Train set: Average loss: 0.0000, Accuracy: 49584/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0452, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "epoch 16\n",
            "Train set: Average loss: 0.0000, Accuracy: 49601/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0443, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "epoch 17\n",
            "Train set: Average loss: 0.0000, Accuracy: 49644/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0427, Accuracy: 9884/10000 (99%)\n",
            "\n",
            "epoch 18\n",
            "Train set: Average loss: 0.0000, Accuracy: 49664/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0422, Accuracy: 9883/10000 (99%)\n",
            "\n",
            "epoch 19\n",
            "Train set: Average loss: 0.0000, Accuracy: 49672/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0409, Accuracy: 9886/10000 (99%)\n",
            "\n",
            "epoch 20\n",
            "Train set: Average loss: 0.0000, Accuracy: 49707/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "epoch 21\n",
            "Train set: Average loss: 0.0000, Accuracy: 49708/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "epoch 22\n",
            "Train set: Average loss: 0.0000, Accuracy: 49727/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0396, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "epoch 23\n",
            "Train set: Average loss: 0.0000, Accuracy: 49745/50000 (99%)\n",
            "\n",
            "Test set: Average loss: 0.0377, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "epoch 24\n",
            "Train set: Average loss: 0.0000, Accuracy: 49760/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0378, Accuracy: 9894/10000 (99%)\n",
            "\n",
            "epoch 25\n",
            "Train set: Average loss: 0.0000, Accuracy: 49776/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0376, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "epoch 26\n",
            "Train set: Average loss: 0.0000, Accuracy: 49788/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 9894/10000 (99%)\n",
            "\n",
            "epoch 27\n",
            "Train set: Average loss: 0.0000, Accuracy: 49811/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "epoch 28\n",
            "Train set: Average loss: 0.0000, Accuracy: 49810/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0360, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "epoch 29\n",
            "Train set: Average loss: 0.0000, Accuracy: 49835/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9896/10000 (99%)\n",
            "\n",
            "epoch 30\n",
            "Train set: Average loss: 0.0000, Accuracy: 49843/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0355, Accuracy: 9898/10000 (99%)\n",
            "\n",
            "epoch 31\n",
            "Train set: Average loss: 0.0000, Accuracy: 49838/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0361, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "epoch 32\n",
            "Train set: Average loss: 0.0000, Accuracy: 49850/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "epoch 33\n",
            "Train set: Average loss: 0.0000, Accuracy: 49859/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 34\n",
            "Train set: Average loss: 0.0000, Accuracy: 49881/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 35\n",
            "Train set: Average loss: 0.0000, Accuracy: 49876/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "epoch 36\n",
            "Train set: Average loss: 0.0000, Accuracy: 49886/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0340, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "epoch 37\n",
            "Train set: Average loss: 0.0000, Accuracy: 49897/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0336, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "epoch 38\n",
            "Train set: Average loss: 0.0000, Accuracy: 49895/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0332, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 39\n",
            "Train set: Average loss: 0.0000, Accuracy: 49910/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 40\n",
            "Train set: Average loss: 0.0000, Accuracy: 49910/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0328, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "epoch 41\n",
            "Train set: Average loss: 0.0000, Accuracy: 49909/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0329, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 42\n",
            "Train set: Average loss: 0.0000, Accuracy: 49924/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 43\n",
            "Train set: Average loss: 0.0000, Accuracy: 49925/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0325, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 44\n",
            "Train set: Average loss: 0.0000, Accuracy: 49932/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 45\n",
            "Train set: Average loss: 0.0000, Accuracy: 49942/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0328, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "epoch 46\n",
            "Train set: Average loss: 0.0000, Accuracy: 49941/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "epoch 47\n",
            "Train set: Average loss: 0.0000, Accuracy: 49937/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 48\n",
            "Train set: Average loss: 0.0000, Accuracy: 49942/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0330, Accuracy: 9899/10000 (99%)\n",
            "\n",
            "epoch 49\n",
            "Train set: Average loss: 0.0000, Accuracy: 49950/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 50\n",
            "Train set: Average loss: 0.0000, Accuracy: 49951/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0322, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "epoch 51\n",
            "Train set: Average loss: 0.0000, Accuracy: 49945/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "epoch 52\n",
            "Train set: Average loss: 0.0000, Accuracy: 49956/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 53\n",
            "Train set: Average loss: 0.0000, Accuracy: 49962/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 54\n",
            "Train set: Average loss: 0.0000, Accuracy: 49961/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 55\n",
            "Train set: Average loss: 0.0000, Accuracy: 49972/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 56\n",
            "Train set: Average loss: 0.0000, Accuracy: 49969/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 57\n",
            "Train set: Average loss: 0.0000, Accuracy: 49962/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 58\n",
            "Train set: Average loss: 0.0000, Accuracy: 49968/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 59\n",
            "Train set: Average loss: 0.0000, Accuracy: 49963/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0315, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 60\n",
            "Train set: Average loss: 0.0000, Accuracy: 49972/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 61\n",
            "Train set: Average loss: 0.0000, Accuracy: 49971/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 62\n",
            "Train set: Average loss: 0.0000, Accuracy: 49977/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 63\n",
            "Train set: Average loss: 0.0000, Accuracy: 49974/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 64\n",
            "Train set: Average loss: 0.0000, Accuracy: 49977/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9903/10000 (99%)\n",
            "\n",
            "epoch 65\n",
            "Train set: Average loss: 0.0000, Accuracy: 49973/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 66\n",
            "Train set: Average loss: 0.0000, Accuracy: 49981/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 67\n",
            "Train set: Average loss: 0.0000, Accuracy: 49982/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 68\n",
            "Train set: Average loss: 0.0000, Accuracy: 49984/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 69\n",
            "Train set: Average loss: 0.0000, Accuracy: 49981/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 70\n",
            "Train set: Average loss: 0.0000, Accuracy: 49984/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 71\n",
            "Train set: Average loss: 0.0000, Accuracy: 49982/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 72\n",
            "Train set: Average loss: 0.0000, Accuracy: 49991/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "epoch 73\n",
            "Train set: Average loss: 0.0000, Accuracy: 49988/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 74\n",
            "Train set: Average loss: 0.0000, Accuracy: 49987/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 75\n",
            "Train set: Average loss: 0.0000, Accuracy: 49991/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 76\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "epoch 77\n",
            "Train set: Average loss: 0.0000, Accuracy: 49990/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 78\n",
            "Train set: Average loss: 0.0000, Accuracy: 49996/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0302, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 79\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "epoch 80\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 81\n",
            "Train set: Average loss: 0.0000, Accuracy: 49987/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 82\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 83\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "epoch 84\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0300, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 85\n",
            "Train set: Average loss: 0.0000, Accuracy: 49991/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 86\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 87\n",
            "Train set: Average loss: 0.0000, Accuracy: 49992/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 88\n",
            "Train set: Average loss: 0.0000, Accuracy: 49991/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 89\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "epoch 90\n",
            "Train set: Average loss: 0.0000, Accuracy: 49997/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 91\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "epoch 92\n",
            "Train set: Average loss: 0.0000, Accuracy: 49993/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 93\n",
            "Train set: Average loss: 0.0000, Accuracy: 49994/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "epoch 94\n",
            "Train set: Average loss: 0.0000, Accuracy: 49990/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 95\n",
            "Train set: Average loss: 0.0000, Accuracy: 49997/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 96\n",
            "Train set: Average loss: 0.0000, Accuracy: 49996/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0301, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "epoch 97\n",
            "Train set: Average loss: 0.0000, Accuracy: 49995/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0304, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "epoch 98\n",
            "Train set: Average loss: 0.0000, Accuracy: 49996/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0303, Accuracy: 9908/10000 (99%)\n",
            "\n",
            "epoch 99\n",
            "Train set: Average loss: 0.0000, Accuracy: 49997/50000 (100%)\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9910/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b68cvQJ-e2pD",
        "colab_type": "code",
        "outputId": "33351980-1db1-41ac-b1b9-5210b0910076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# PLOTTING\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "for filename in os.listdir(savedir):\n",
        "    if filename.endswith('.pkl'):\n",
        "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
        "            results = pickle.load(fin)\n",
        "            ax1.plot(results['loss'])\n",
        "            ax1.set_ylabel('cross entropy')\n",
        "            ax1.set_xlabel('epochs')\n",
        "            \n",
        "            ax2.plot(results['accuracy'], label = \"validation set\")\n",
        "            ax2.plot(results['t_accuracy'], label = \"training set\")\n",
        "            ax2.set_ylabel('accuracy')\n",
        "            ax2.set_xlabel('epochs')\n",
        "            \n",
        "plt.legend()\n",
        "plt.savefig(\"cnn3.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAEJCAYAAADVUVCbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "kHOumrR5uHbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ]
    },
    {
      "metadata": {
        "id": "YAwgN237uJSI",
        "colab_type": "code",
        "outputId": "78dd5176-4c06-43f9-c17a-88bb5361d1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# Chosen model\n",
        "chosen_model = model_evol[5]\n",
        "test(model, test_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: Average loss: 0.0305, Accuracy: 9910/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.030497188472747804, 0.991)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}